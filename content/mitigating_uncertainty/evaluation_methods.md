---
title: "Mitigating uncertainty in experimental and quasi-experimental evaluation methods"
draft: false
weight: 3
---

An important consideration in experimental and quasi-experimental evaluation methods is whether the counterfactual and intervention group are comparable. This depends on the collection of comparable data from both the intervention and control groups to ensure you can measure the intervention effect. In addition, the intervention effect must be large enough to be distinguished from noise in the data. Details of how to design a counterfactual for quasi-experimental and experimental studies can be found in the [Magenta Book](https://www.gov.uk/government/publications/the-magenta-book).

Where it is not possible to run an experimental study to measure impact, you should select a quasi-experimental method based on the availability of a comparison group. Details of which methods are appropriate for different situations are explained in section 3.5 of the [Magenta Book](https://www.gov.uk/government/publications/the-magenta-book).

Where no comparison group is available, as in the case of analysing the impact of a global event like the COVID-19 pandemic or 2008 economic crisis, you can use an interrupted time-series approach. However, this is only appropriate where changes are sudden. This approach option is preferred to binary before-after comparisons which can be misleading as they present data out of the context of underlying trends.

Ensuring you have a solid grounding in the theory underpinning your intervention is essential for an effective evaluation design. Developing a comprehensive theory of change to explain how inputs of the intervention lead to outputs, outcomes and impact and testing it against existing evidence will help identify what data is needed for an impact evaluation. Having a robust, high quality theory of change will also ensure that you do not miss any key indicators and alert you to any potential unintended consequences that may need to be measured.

There is a useful [logic models guide on gov.uk](https://www.gov.uk/guidance/evaluation-in-health-and-wellbeing-creating-a-logic-model). This goes through the process of creating a logic model and provides examples and templates. There is also some good guidance on developing and using logic models from the [Nuffield Trust](https://www.nuffieldtrust.org.uk/sites/default/files/2019-02/stephanie-kumpunen-and-muna-sheikh.pdf) and a [step by step guide from the US CDC](https://www.cdc.gov/evaluation/).

A baseline, or starting point, can also serve as a benchmark against which future progress and effects of an intervention can be assessed. Comparing estimated effect sizes to effect sizes from other studies in different contexts can be useful for setting realistic expectations of effects and thereby guide the research design (e.g. required sample size, whether effects are homogeneous or heterogeneous). Benchmarking can also help to understand similarities and differences between the effects of interventions in different contexts.

Where data is limited or the intervention will produce small or unpredictable effects, theory-based evaluation could be an alternative. Theory-based impact evaluations make conclusions about the effect of an intervention through testing the causal pathways through which the change is brought about. This method assesses whether the evidence is sufficient to support these causal pathways and that alternative explanations can be ruled out. The Magenta Book provides further guidance on theory-based evaluation.